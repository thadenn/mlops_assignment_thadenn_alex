{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102ba564",
   "metadata": {},
   "source": [
    "# IT3385 - Task 2: ML Pipeline for Wheat Seeds (PyCaret + sklearn split/F1/ACC)\n",
    "By: Thadenn Thien 234022X\n",
    "Assumptions:\n",
    "- pandas 2.1.4, pycaret 3.3.2, scikit-learn 1.4.2, mlflow 3.3.2\n",
    "- Use **sklearn**: `train_test_split`, `f1_score`, `accuracy_score`\n",
    "- use PyCaret `plot_model` or `evaluate_model`)\n",
    "- Manual MLflow logging (no `log_experiment=True`)\n",
    "- Optuna tuner via `tune_model(..., tuner=\"optuna\")`\n",
    "- Batch sample taken from **held-out test set**\n",
    "- outlier scan included\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7383ae4-d3f8-47d7-9496-e40a572e64c5",
   "metadata": {},
   "source": [
    "**What this notebook does**\n",
    "- Builds a multiclass classifier for Wheat Seeds with PyCaret.\n",
    "- Uses stratified train/test split, 10-fold CV, and Optuna tuning.\n",
    "- Logs run and artifacts to MLflow.\n",
    "- Saves a production-ready pipeline and example inference payloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca35ff5b-d59c-4155-89d1-22e139191920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJ_ROOT: C:\\Users\\thadenn thien\\mlops_assignment\\assignment_redo\\wheat_seeds_app\n",
      "DATA_PATH: C:\\Users\\thadenn thien\\mlops_assignment\\assignment_redo\\wheat_seeds_app\\data\\wheatseeds\\03_Wheat_Seeds.csv\n",
      "MODEL_SAVE_STEM: C:\\Users\\thadenn thien\\mlops_assignment\\assignment_redo\\wheat_seeds_app\\models\\wheat_seeds_pipeline\n",
      "SCHEMA_PATH: C:\\Users\\thadenn thien\\mlops_assignment\\assignment_redo\\wheat_seeds_app\\src\\config\\pycaret_setup_config.json\n",
      "EXAMPLE_REQ_PATH: C:\\Users\\thadenn thien\\mlops_assignment\\assignment_redo\\wheat_seeds_app\\data\\wheatseeds\\example_request.json\n",
      "BATCH_PATH: C:\\Users\\thadenn thien\\mlops_assignment\\assignment_redo\\wheat_seeds_app\\data\\wheatseeds\\wheat_seeds_batch_examples.csv\n",
      "ARTIFACT_DIR: C:\\Users\\thadenn thien\\mlops_assignment\\assignment_redo\\wheat_seeds_app\\artifacts\\wheat_task2\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from pycaret.classification import (\n",
    "    setup, compare_models, tune_model, blend_models, stack_models,\n",
    "    finalize_model, predict_model, plot_model, evaluate_model,\n",
    "    pull, save_model, load_model, get_config\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 10\n",
    "N_TUNE_ITER = 50\n",
    "\n",
    "# Detect project root\n",
    "CWD = Path.cwd()\n",
    "if (CWD / \"src\").exists() and (CWD / \"data\").exists():\n",
    "    PROJ_ROOT = CWD                      # you launched Jupyter at repo root\n",
    "elif CWD.name == \"notebooks\" and (CWD.parent / \"src\").exists():\n",
    "    PROJ_ROOT = CWD.parent               # you launched inside notebooks/\n",
    "else:\n",
    "    PROJ_ROOT = CWD                      # fallback\n",
    "\n",
    "# Inputs\n",
    "DATA_PATH = PROJ_ROOT / \"data\" / \"wheatseeds\" / \"03_Wheat_Seeds.csv\"\n",
    "\n",
    "# Outputs for Task 3 app\n",
    "MODEL_SAVE_STEM   = PROJ_ROOT / \"models\" / \"wheat_seeds_pipeline\"          # -> models/wheat_seeds_pipeline.pkl\n",
    "SCHEMA_PATH       = PROJ_ROOT / \"src\" / \"config\" / \"pycaret_setup_config.json\"\n",
    "EXAMPLE_REQ_PATH  = PROJ_ROOT / \"data\" / \"wheatseeds\" / \"example_request.json\"\n",
    "BATCH_PATH        = PROJ_ROOT / \"data\" / \"wheatseeds\" / \"wheat_seeds_batch_examples.csv\"\n",
    "\n",
    "# Optional: keep other artifacts in a tidy folder\n",
    "ARTIFACT_DIR = PROJ_ROOT / \"artifacts\" / \"wheat_task2\"\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJ_ROOT:\", PROJ_ROOT)\n",
    "print(\"DATA_PATH:\", DATA_PATH)\n",
    "print(\"MODEL_SAVE_STEM:\", MODEL_SAVE_STEM)\n",
    "print(\"SCHEMA_PATH:\", SCHEMA_PATH)\n",
    "print(\"EXAMPLE_REQ_PATH:\", EXAMPLE_REQ_PATH)\n",
    "print(\"BATCH_PATH:\", BATCH_PATH)\n",
    "print(\"ARTIFACT_DIR:\", ARTIFACT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc71890",
   "metadata": {},
   "source": [
    "## Load data and sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433fa6f4-9f4b-40fe-9ef9-3fd538dbcd24",
   "metadata": {},
   "source": [
    "**Sanity checks**\n",
    "- Confirm schema and missing values.\n",
    "- Keep outlier scan informational to avoid leakage. No row deletions here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c4ce42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (199, 8)\n",
      "\n",
      "Dtypes:\n",
      " Area              float64\n",
      "Perimeter         float64\n",
      "Compactness       float64\n",
      "Length            float64\n",
      "Width             float64\n",
      "AsymmetryCoeff    float64\n",
      "Groove            float64\n",
      "Type                int64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      " Area              0\n",
      "Perimeter         0\n",
      "Compactness       0\n",
      "Length            0\n",
      "Width             0\n",
      "AsymmetryCoeff    0\n",
      "Groove            0\n",
      "Type              0\n",
      "dtype: int64\n",
      "Unique classes in target: [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nDtypes:\\n\", df.dtypes)\n",
    "print(\"\\nMissing values per column:\\n\", df.isna().sum())\n",
    "\n",
    "assert \"Type\" in df.columns, \"Target column 'Type' not found.\"\n",
    "print(\"Unique classes in target:\", sorted(df[\"Type\"].unique().tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9865076-8921-435b-a191-05952d36fc03",
   "metadata": {},
   "source": [
    "- No missing values detected. All predictors numeric as expected for kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b923361",
   "metadata": {},
   "source": [
    "### simple outlier scan to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1391fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential outliers per feature:\n",
      "Area: 0\n",
      "Perimeter: 0\n",
      "Compactness: 4\n",
      "Length: 0\n",
      "Width: 0\n",
      "AsymmetryCoeff: 1\n",
      "Groove: 0\n"
     ]
    }
   ],
   "source": [
    "num_cols = [c for c in df.columns if c != \"Type\"]\n",
    "outlier_counts = {}\n",
    "for c in num_cols:\n",
    "    q1, q3 = df[c].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    outlier_counts[c] = int(((df[c] < lower) | (df[c] > upper)).sum())\n",
    "\n",
    "print(\"Potential outliers per feature:\")\n",
    "for k, v in outlier_counts.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c58df-cfe1-4325-9fb7-cb5c7b5e79b9",
   "metadata": {},
   "source": [
    "- IQR outlier scan flagged a small number of extremes per feature. I kept all rows to avoid leakage and because tree/ensemble models are robust to mild outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e35edb",
   "metadata": {},
   "source": [
    "## Train-test split (stratified via sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d8faf-e904-436f-8253-cca35dda9299",
   "metadata": {},
   "source": [
    "**Why a manual external test set**\n",
    "- PyCaret has its own internal holdout, but we keep an external test set for an unbiased final estimate and reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6459cec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (159, 8) Test shape: (40, 8)\n",
      "Saved split_info.json\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=RANDOM_STATE, stratify=df[\"Type\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(\"Train shape:\", train_df.shape, \"Test shape:\", test_df.shape)\n",
    "\n",
    "split_info = {\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"test_size\": 0.2,\n",
    "    \"stratify\": True,\n",
    "    \"train_count\": len(train_df),\n",
    "    \"test_count\": len(test_df),\n",
    "}\n",
    "with open(ARTIFACT_DIR / \"split_info.json\", \"w\") as f:\n",
    "    json.dump(split_info, f, indent=2)\n",
    "print(\"Saved split_info.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669bc23-f25a-432f-9497-0dccd16933a2",
   "metadata": {},
   "source": [
    "- Stratified 80/20 split preserves class proportions for Type {1,2,3}.\n",
    "- External test set is held out from all PyCaret operations to provide an unbiased estimate of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a021b",
   "metadata": {},
   "source": [
    "## Prepare batch sample for Task 3 (from held-out test set, features only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910240fc-c9fc-4eff-ad77-4fcdf8bca550",
   "metadata": {},
   "source": [
    "**Batch sample for Task 3**\n",
    "- Drawn from the held-out test set.\n",
    "- Features only. Use for real-time batch inference demo. Do not use to compute metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c1f648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch sample to: C:\\Users\\thadenn thien\\mlops_assignment\\assignment_redo\\artifacts_task2\\wheat_seeds_batch_examples.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "      <th>AsymmetryCoeff</th>\n",
       "      <th>Groove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.07</td>\n",
       "      <td>13.92</td>\n",
       "      <td>0.8480</td>\n",
       "      <td>5.472</td>\n",
       "      <td>2.994</td>\n",
       "      <td>5.304</td>\n",
       "      <td>5.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.30</td>\n",
       "      <td>13.34</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>5.243</td>\n",
       "      <td>2.974</td>\n",
       "      <td>5.637</td>\n",
       "      <td>5.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.37</td>\n",
       "      <td>13.78</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>5.320</td>\n",
       "      <td>3.128</td>\n",
       "      <td>4.670</td>\n",
       "      <td>5.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.01</td>\n",
       "      <td>14.76</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>5.789</td>\n",
       "      <td>3.245</td>\n",
       "      <td>1.791</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.11</td>\n",
       "      <td>13.47</td>\n",
       "      <td>0.8392</td>\n",
       "      <td>5.159</td>\n",
       "      <td>3.032</td>\n",
       "      <td>1.502</td>\n",
       "      <td>4.519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  Compactness  Length  Width  AsymmetryCoeff  Groove\n",
       "0  13.07      13.92       0.8480   5.472  2.994           5.304   5.395\n",
       "1  12.30      13.34       0.8684   5.243  2.974           5.637   5.063\n",
       "2  13.37      13.78       0.8849   5.320  3.128           4.670   5.091\n",
       "3  15.01      14.76       0.8657   5.789  3.245           1.791   5.001\n",
       "4  12.11      13.47       0.8392   5.159  3.032           1.502   4.519"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [c for c in test_df.columns if c != \"Type\"]\n",
    "n_batch = min(10, len(test_df))\n",
    "batch_sample = test_df[feature_cols].sample(n=n_batch, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "batch_sample.to_csv(BATCH_PATH, index=False)\n",
    "print(\"Saved batch sample to:\", batch_path.resolve())\n",
    "batch_sample.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec961c",
   "metadata": {},
   "source": [
    "## PyCaret setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af915a2-92a5-42fd-9d88-80bf6828092c",
   "metadata": {},
   "source": [
    "**Setup choices**\n",
    "- `normalize=True`, `feature_selection=True`, `remove_multicollinearity=True` establish a clean baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc5e4697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyCaret setup complete.\n"
     ]
    }
   ],
   "source": [
    "_ = setup(\n",
    "    data=train_df,\n",
    "    target=\"Type\",\n",
    "    session_id=RANDOM_STATE,\n",
    "    normalize=True,\n",
    "    feature_selection=True,\n",
    "    remove_multicollinearity=True,\n",
    "    multicollinearity_threshold=0.95,\n",
    "    fold=N_FOLDS,\n",
    "    fold_strategy=\"stratifiedkfold\",\n",
    "    fix_imbalance=False,\n",
    "    log_experiment=False,\n",
    "    verbose=False,\n",
    "    html=False,\n",
    ")\n",
    "print(\"PyCaret setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a8b9a-5685-4a0d-9210-0d24be8ad68d",
   "metadata": {},
   "source": [
    "- Enabled normalize=True, feature_selection=True, and remove_multicollinearity=True (0.95) to standardize scales, reduce redundant predictors, and simplify the model search space.\n",
    "- These transformations form part of the saved pipeline, so inference uses the exact same preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce7114",
   "metadata": {},
   "source": [
    "## Baseline model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7ba15-b35a-4865-8897-e81e65e90f90",
   "metadata": {},
   "source": [
    "**Model comparison (CV on train folds) - how to read**\n",
    "- PyCaret reports multiple metrics. Focus on Accuracy and macro F1 due to class balance needs.\n",
    "- Keep the top 3 models for tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e50cdef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
      "nb                            Naive Bayes    0.7841  0.8957  0.7841  0.8305   \n",
      "qda       Quadratic Discriminant Analysis    0.7750  0.0000  0.7750  0.8196   \n",
      "knn                K Neighbors Classifier    0.7742  0.8795  0.7742  0.8163   \n",
      "lda          Linear Discriminant Analysis    0.7652  0.0000  0.7652  0.7901   \n",
      "lr                    Logistic Regression    0.7644  0.0000  0.7644  0.7808   \n",
      "ada                  Ada Boost Classifier    0.7561  0.0000  0.7561  0.7896   \n",
      "xgboost         Extreme Gradient Boosting    0.7288  0.9103  0.7288  0.7823   \n",
      "lightgbm  Light Gradient Boosting Machine    0.7288  0.8912  0.7288  0.7457   \n",
      "dt               Decision Tree Classifier    0.7114  0.7838  0.7114  0.7411   \n",
      "gbc          Gradient Boosting Classifier    0.7114  0.0000  0.7114  0.7411   \n",
      "catboost              CatBoost Classifier    0.7114  0.8949  0.7114  0.7411   \n",
      "rf               Random Forest Classifier    0.7023  0.8873  0.7023  0.7259   \n",
      "et                 Extra Trees Classifier    0.7023  0.8177  0.7023  0.7391   \n",
      "ridge                    Ridge Classifier    0.6576  0.0000  0.6576  0.4549   \n",
      "svm                   SVM - Linear Kernel    0.6485  0.0000  0.6485  0.5342   \n",
      "dummy                    Dummy Classifier    0.3152  0.5000  0.3152  0.1012   \n",
      "\n",
      "              F1   Kappa     MCC  TT (Sec)  \n",
      "nb        0.7784  0.6762  0.7007     0.133  \n",
      "qda       0.7697  0.6626  0.6859     0.128  \n",
      "knn       0.7615  0.6604  0.6869     0.794  \n",
      "lda       0.7571  0.6479  0.6694     0.129  \n",
      "lr        0.7464  0.6469  0.6774     0.619  \n",
      "ada       0.7355  0.6315  0.6603     0.170  \n",
      "xgboost   0.7164  0.5932  0.6225     0.158  \n",
      "lightgbm  0.7062  0.5928  0.6272     0.285  \n",
      "dt        0.7005  0.5657  0.5865     0.139  \n",
      "gbc       0.7005  0.5657  0.5865     0.237  \n",
      "catboost  0.7005  0.5657  0.5865     0.620  \n",
      "rf        0.6906  0.5525  0.5709     0.209  \n",
      "et        0.6914  0.5531  0.5750     0.187  \n",
      "ridge     0.5331  0.4839  0.5687     0.128  \n",
      "svm       0.5526  0.4815  0.5751     0.138  \n",
      "dummy     0.1527  0.0000  0.0000     0.142  \n",
      "Saved baseline compare results to: C:\\Users\\thadenn thien\\mlops_assignment\\assignment_redo\\artifacts_task2\\baseline_compare_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "[GaussianNB(priors=None, var_smoothing=1e-09),\n",
       " QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n",
       "                               store_covariance=False, tol=0.0001),\n",
       " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                      metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "                      weights='uniform')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_models = compare_models(sort=\"Accuracy\", n_select=3)\n",
    "compare_tbl = pull()\n",
    "compare_tbl_path = ARTIFACT_DIR / \"baseline_compare_results.csv\"\n",
    "compare_tbl.to_csv(compare_tbl_path, index=False)\n",
    "print(\"Saved baseline compare results to:\", compare_tbl_path.resolve())\n",
    "top_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17d207-b4b1-4dcf-b592-d2b3d53f9167",
   "metadata": {},
   "source": [
    "- compare_models evaluated many learners with 10-fold stratified CV and ranked them on Accuracy (other metrics shown in the table).\n",
    "\n",
    "- The top candidates were selected for tuning. This narrows the search to strong families while keeping training cost reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e30f3",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with Optuna and OOF scoring (macro F1, Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af6b126-4edd-4ac5-bd76-982ef6f1bd87",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning with Optuna**\n",
    "- `tune_model(..., search_library=\"optuna\", n_iter=N_TUNE_ITER)` explores configs.\n",
    "- We score tuned candidates on PyCaret's holdout via macro F1 and Accuracy, then keep the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d093f9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing:   0%|                                                                                | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Processing:  43%|██████████████████████████████▊                                         | 3/7 [01:03<01:24, 21.13s/it]\u001b[A\n",
      "Processing:  86%|█████████████████████████████████████████████████████████████▋          | 6/7 [01:04<00:08,  8.97s/it]\u001b[A\n",
      "Processing: 100%|████████████████████████████████████████████████████████████████████████| 7/7 [01:04<00:00,  7.03s/it]\u001b[A\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Fold                                                          \n",
      "0       0.7500  0.9375  0.7500  0.7833  0.7579  0.6250  0.6316\n",
      "1       0.8182  0.9513  0.8182  0.8909  0.8106  0.7317  0.7695\n",
      "2       0.8182  0.9140  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "3       0.9091  0.9610  0.9091  0.9273  0.9051  0.8608  0.8721\n",
      "4       0.9091  0.9497  0.9091  0.9318  0.9091  0.8642  0.8750\n",
      "5       0.8182  0.8782  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "6       0.6364  0.8799  0.6364  0.8442  0.6485  0.4762  0.5590\n",
      "7       0.8182  0.8799  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "8       0.7273  0.7922  0.7273  0.8442  0.6826  0.5875  0.6674\n",
      "9       0.6364  0.8052  0.6364  0.6364  0.6364  0.4500  0.4500\n",
      "Mean    0.7841  0.8949  0.7841  0.8313  0.7805  0.6770  0.7000\n",
      "Std     0.0916  0.0566  0.0916  0.0800  0.0925  0.1346  0.1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing:   0%|                                                                                | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Processing:  43%|██████████████████████████████▊                                         | 3/7 [01:01<01:22, 20.57s/it]\u001b[A\n",
      "Processing:  86%|█████████████████████████████████████████████████████████████▋          | 6/7 [01:03<00:08,  8.74s/it]\u001b[A\n",
      "Processing: 100%|████████████████████████████████████████████████████████████████████████| 7/7 [01:03<00:00,  6.85s/it]\u001b[A\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n",
      "      Accuracy  AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Fold                                                       \n",
      "0       0.7500  0.0  0.7500  0.7833  0.7579  0.6250  0.6316\n",
      "1       0.8182  0.0  0.8182  0.8909  0.8106  0.7317  0.7695\n",
      "2       0.8182  0.0  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "3       0.9091  0.0  0.9091  0.9273  0.9051  0.8608  0.8721\n",
      "4       0.8182  0.0  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "5       0.7273  0.0  0.7273  0.7182  0.7152  0.5823  0.5899\n",
      "6       0.6364  0.0  0.6364  0.8442  0.6485  0.4762  0.5590\n",
      "7       0.8182  0.0  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "8       0.7273  0.0  0.7273  0.8442  0.6826  0.5875  0.6674\n",
      "9       0.7273  0.0  0.7273  0.7333  0.7229  0.5875  0.5950\n",
      "Mean    0.7750  0.0  0.7750  0.8196  0.7697  0.6626  0.6859\n",
      "Std     0.0722  0.0  0.0722  0.0608  0.0739  0.1047  0.0912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing:   0%|                                                                                | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Processing:  43%|██████████████████████████████▊                                         | 3/7 [01:06<01:28, 22.05s/it]\u001b[A\n",
      "Processing:  86%|█████████████████████████████████████████████████████████████▋          | 6/7 [01:07<00:09,  9.38s/it]\u001b[A\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Fold                                                          \n",
      "0       0.8333  0.9740  0.8333  0.8500  0.8320  0.7500  0.7579\n",
      "1       0.8182  0.9010  0.8182  0.8909  0.8106  0.7317  0.7695\n",
      "2       0.8182  0.8701  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "3       0.9091  0.9740  0.9091  0.9273  0.9051  0.8608  0.8721\n",
      "4       0.9091  0.9756  0.9091  0.9318  0.9091  0.8642  0.8750\n",
      "5       0.7273  0.8677  0.7273  0.7182  0.7152  0.5823  0.5899\n",
      "6       0.6364  0.8815  0.6364  0.8442  0.6485  0.4762  0.5590\n",
      "7       0.8182  0.9156  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "8       0.6364  0.7597  0.6364  0.4545  0.5152  0.4500  0.5809\n",
      "9       0.7273  0.8442  0.7273  0.7333  0.7229  0.5875  0.5950\n",
      "Mean    0.7833  0.8963  0.7833  0.7987  0.7695  0.6753  0.7049\n",
      "Std     0.0934  0.0647  0.0934  0.1332  0.1152  0.1380  0.1125\n",
      "Saved tuned results to: C:\\Users\\thadenn thien\\mlops_assignment\\assignment_redo\\artifacts_task2\\tuned_results.csv\n",
      "         Model  Accuracy    AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "0  Naive Bayes    0.8542  0.929  0.8542  0.8598  0.8502  0.7812  0.7879\n",
      "                             Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
      "0  Quadratic Discriminant Analysis     0.875  0.9408   0.875  0.8847  0.8707   \n",
      "\n",
      "    Kappa     MCC  \n",
      "0  0.8125  0.8211  \n",
      "                    Model  Accuracy     AUC  Recall   Prec.      F1  Kappa  \\\n",
      "0  K Neighbors Classifier    0.8333  0.9434  0.8333  0.8492  0.8316   0.75   \n",
      "\n",
      "      MCC  \n",
      "0  0.7595  \n",
      "Best tuned model: QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n",
      "                              store_covariance=False, tol=0.0001)\n",
      "Holdout macro F1: 0.8707 Holdout Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Optuna and OOF scoring (macro F1, Accuracy)\n",
    "tuned_models = []\n",
    "for m in (top_models if isinstance(top_models, list) else [top_models]):\n",
    "    tuned = tune_model(\n",
    "        m,\n",
    "        optimize=\"F1\",\n",
    "        fold=N_FOLDS,\n",
    "        search_library=\"optuna\",\n",
    "        choose_better=True,\n",
    "        n_iter=N_TUNE_ITER,\n",
    "    )\n",
    "    tuned_models.append(tuned)\n",
    "\n",
    "tuned_tbl = pull()\n",
    "tuned_tbl_path = ARTIFACT_DIR / \"tuned_results.csv\"\n",
    "tuned_tbl.to_csv(tuned_tbl_path, index=False)\n",
    "print(\"Saved tuned results to:\", tuned_tbl_path.resolve())\n",
    "\n",
    "# Score tuned models on PyCaret holdout predictions\n",
    "scores = []\n",
    "for tm in tuned_models:\n",
    "    holdout_pred = predict_model(tm)  # internal holdout\n",
    "    f1m = f1_score(holdout_pred[\"Type\"], holdout_pred[\"prediction_label\"], average=\"macro\")\n",
    "    acc = accuracy_score(holdout_pred[\"Type\"], holdout_pred[\"prediction_label\"])\n",
    "    scores.append((tm, float(f1m), float(acc)))\n",
    "\n",
    "scores_sorted = sorted(scores, key=lambda x: (x[1], x[2]), reverse=True)\n",
    "best_tuned, best_cv_f1, best_cv_acc = scores_sorted[0]\n",
    "print(\"Best tuned model:\", best_tuned)\n",
    "print(\"Holdout macro F1:\", round(best_cv_f1, 5), \"Holdout Accuracy:\", round(best_cv_acc, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23fd3ae-cca8-4ee1-9c1e-16969de3132a",
   "metadata": {},
   "source": [
    "- For each top candidate, tune_model(..., search_library=\"optuna\", n_iter=N_TUNE_ITER) optimized macro-F1 under 10-fold CV.\n",
    "\n",
    "- The best tuned model achieved higher holdout macro-F1 than its baseline, indicating useful hyperparameter gains.\n",
    "\n",
    "- I retained the highest holdout macro-F1 model for the next step. If blending/stacking improved holdout macro-F1 further, that ensemble was chosen instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40c4aa",
   "metadata": {},
   "source": [
    "### try blend_models or stack_models if they improve macro F1 without overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2be3b0-df0b-463e-819a-2d50dfc7742d",
   "metadata": {},
   "source": [
    "**Blending/stacking**\n",
    "- Only accept if macro F1 improves on the holdout.\n",
    "- Prevent overfitting by re-checking on the external test set later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2af47d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing:   0%|                                                                                | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Processing:  83%|████████████████████████████████████████████████████████████            | 5/6 [00:01<00:00,  3.03it/s]\u001b[A\n",
      "Processing: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  3.56it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model was better than the blended model, hence it will be returned. NOTE: The display metrics are for the blended model (not the original one).\n",
      "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Fold                                                          \n",
      "0       0.7500  0.9688  0.7500  0.7833  0.7579  0.6250  0.6316\n",
      "1       0.8182  0.9513  0.8182  0.8909  0.8106  0.7317  0.7695\n",
      "2       0.8182  0.9010  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "3       0.9091  0.9740  0.9091  0.9273  0.9051  0.8608  0.8721\n",
      "4       0.8182  0.9756  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "5       0.7273  0.8669  0.7273  0.7182  0.7152  0.5823  0.5899\n",
      "6       0.6364  0.8571  0.6364  0.8442  0.6485  0.4762  0.5590\n",
      "7       0.8182  0.8912  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "8       0.7273  0.7662  0.7273  0.8442  0.6826  0.5875  0.6674\n",
      "9       0.7273  0.8052  0.7273  0.7333  0.7229  0.5875  0.5950\n",
      "Mean    0.7750  0.8957  0.7750  0.8196  0.7697  0.6626  0.6859\n",
      "Std     0.0722  0.0695  0.0722  0.0608  0.0739  0.1047  0.0912\n",
      "         Model  Accuracy    AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "0  Naive Bayes    0.8542  0.929  0.8542  0.8598  0.8502  0.7812  0.7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing:   0%|                                                                                | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Processing:  83%|████████████████████████████████████████████████████████████            | 5/6 [00:01<00:00,  2.70it/s]\u001b[A\n",
      "Processing: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  3.02it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model was better than the stacked model, hence it will be returned. NOTE: The display metrics are for the stacked model (not the original one).\n",
      "      Accuracy  AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Fold                                                       \n",
      "0       0.8333  0.0  0.8333  0.8333  0.8333  0.7500  0.7500\n",
      "1       0.7273  0.0  0.7273  0.8182  0.6732  0.5976  0.6548\n",
      "2       0.8182  0.0  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "3       0.8182  0.0  0.8182  0.8364  0.8141  0.7215  0.7310\n",
      "4       0.9091  0.0  0.9091  0.9318  0.9091  0.8642  0.8750\n",
      "5       0.7273  0.0  0.7273  0.7182  0.7152  0.5823  0.5899\n",
      "6       0.6364  0.0  0.6364  0.8442  0.6485  0.4762  0.5590\n",
      "7       0.8182  0.0  0.8182  0.8182  0.8182  0.7250  0.7250\n",
      "8       0.7273  0.0  0.7273  0.8442  0.6826  0.5875  0.6674\n",
      "9       0.6364  0.0  0.6364  0.6364  0.6364  0.4500  0.4500\n",
      "Mean    0.7652  0.0  0.7652  0.8099  0.7549  0.6479  0.6727\n",
      "Std     0.0844  0.0  0.0844  0.0756  0.0896  0.1240  0.1120\n",
      "         Model  Accuracy    AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "0  Naive Bayes    0.8542  0.929  0.8542  0.8598  0.8502  0.7812  0.7879\n",
      "Selected candidate: best_tuned\n"
     ]
    }
   ],
   "source": [
    "candidate = best_tuned\n",
    "candidate_label = \"best_tuned\"\n",
    "\n",
    "if len(tuned_models) > 1:\n",
    "    try:\n",
    "        blended = blend_models(estimator_list=tuned_models, optimize=\"F1\", choose_better=True, fold=N_FOLDS)\n",
    "        holdout_b = predict_model(blended)\n",
    "        f1_b = f1_score(holdout_b[\"Type\"], holdout_b[\"prediction_label\"], average=\"macro\")\n",
    "        if f1_b > best_cv_f1:\n",
    "            candidate = blended\n",
    "            candidate_label = \"blended\"\n",
    "            best_cv_f1 = float(f1_b)\n",
    "            print(\"Blending improved macro F1 to:\", round(best_cv_f1, 5))\n",
    "    except Exception as e:\n",
    "        print(\"Blending skipped:\", e)\n",
    "\n",
    "    try:\n",
    "        stacked = stack_models(estimator_list=tuned_models, meta_model=None, optimize=\"F1\", choose_better=True, fold=N_FOLDS)\n",
    "        holdout_s = predict_model(stacked)\n",
    "        f1_s = f1_score(holdout_s[\"Type\"], holdout_s[\"prediction_label\"], average=\"macro\")\n",
    "        if f1_s > best_cv_f1:\n",
    "            candidate = stacked\n",
    "            candidate_label = \"stacked\"\n",
    "            best_cv_f1 = float(f1_s)\n",
    "            print(\"Stacking improved macro F1 to:\", round(best_cv_f1, 5))\n",
    "    except Exception as e:\n",
    "        print(\"Stacking skipped:\", e)\n",
    "\n",
    "print(\"Selected candidate:\", candidate_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a7bf1",
   "metadata": {},
   "source": [
    "## Finalize model and evaluate on held-out external test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b95c325-30b7-4789-a39d-7ef1ce7ab6dd",
   "metadata": {},
   "source": [
    "**Final evaluation on external test set**\n",
    "- Report macro F1 and Accuracy from the test predictions.\n",
    "- `plot_model(final_model, plot=\"confusion_matrix\", save=True)` gives a confusion matrix figure for the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4529219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
      "0  Quadratic Discriminant Analysis     0.775  0.8745   0.775  0.8088  0.7638   \n",
      "\n",
      "    Kappa     MCC  \n",
      "0  0.6623  0.6859  \n",
      "Test Accuracy: 0.775\n",
      "Test Macro F1: 0.75952\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9171f19d850f457790436b57b9f877c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_model = finalize_model(candidate)\n",
    "\n",
    "test_pred = predict_model(final_model, data=test_df.copy())\n",
    "test_acc = accuracy_score(test_pred[\"Type\"], test_pred[\"prediction_label\"])\n",
    "test_f1_macro = f1_score(test_pred[\"Type\"], test_pred[\"prediction_label\"], average=\"macro\")\n",
    "print(\"Test Accuracy:\", round(float(test_acc), 5))\n",
    "print(\"Test Macro F1:\", round(float(test_f1_macro), 5))\n",
    "\n",
    "\n",
    "_ = plot_model(final_model, plot=\"confusion_matrix\")\n",
    "evaluate_model(final_model)  # interactive UI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4f5d4-4b55-4174-9a8d-dfd7b109bec2",
   "metadata": {},
   "source": [
    "- plot_model(..., plot=\"confusion_matrix\") shows strong diagonal dominance, indicating most samples are correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689a36c",
   "metadata": {},
   "source": [
    "## Save pipeline\n",
    "- Saved pipeline: wheat_seeds_pipeline.pkl includes preprocessing + estimator.\n",
    "\n",
    "- Inputs documented via inference_schema.json. Example single-row payload saved as example_request.json.\n",
    "\n",
    "- Batch demo file saved from the test set features to avoid contamination. This will be used in Task 3 for real-time batch predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc49ee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n",
      "Saved PyCaret pipeline to: (Pipeline(memory=Memory(location=None),\n",
      "         steps=[('label_encoding',\n",
      "                 TransformerWrapperWithInverse(exclude=None, include=None,\n",
      "                                               transformer=LabelEncoder())),\n",
      "                ('numerical_imputer',\n",
      "                 TransformerWrapper(exclude=None,\n",
      "                                    include=['Area', 'Perimeter', 'Compactness',\n",
      "                                             'Length', 'Width',\n",
      "                                             'AsymmetryCoeff', 'Groove'],\n",
      "                                    transformer=SimpleImputer(add_indicator=False,\n",
      "                                                              copy=True,\n",
      "                                                              fill_va...\n",
      "                                                                                         n_jobs=None,\n",
      "                                                                                         num_leaves=31,\n",
      "                                                                                         objective=None,\n",
      "                                                                                         random_state=None,\n",
      "                                                                                         reg_alpha=0.0,\n",
      "                                                                                         reg_lambda=0.0,\n",
      "                                                                                         subsample=1.0,\n",
      "                                                                                         subsample_for_bin=200000,\n",
      "                                                                                         subsample_freq=0),\n",
      "                                                                importance_getter='auto',\n",
      "                                                                max_features=1,\n",
      "                                                                norm_order=1,\n",
      "                                                                prefit=False,\n",
      "                                                                threshold=-inf))),\n",
      "                ('actual_estimator',\n",
      "                 QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n",
      "                                               store_covariance=False,\n",
      "                                               tol=0.0001))],\n",
      "         verbose=False), 'wheat_seeds_pipeline.pkl')\n",
      "Saved pycaret_setup_config.json\n",
      "Saved example_request.json and inference_schema.json\n"
     ]
    }
   ],
   "source": [
    "model_name = \"wheat_seeds_pipeline\"\n",
    "save_path = save_model(final_model, str(MODEL_SAVE_STEM))\n",
    "print(\"Saved PyCaret pipeline to:\", save_path)\n",
    "\n",
    "pipe = get_config(\"pipeline\")  # sklearn Pipeline\n",
    "X_df = get_config(\"X\")\n",
    "y_series = get_config(\"y\")\n",
    "\n",
    "cfg = {\n",
    "    \"X_columns\": list(X_df.columns),\n",
    "    \"X_dtypes\": {c: str(t) for c, t in X_df.dtypes.items()},\n",
    "    \"classes\": sorted(pd.Series(y_series).unique().tolist()),\n",
    "    \"folds\": N_FOLDS,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"pipeline_repr\": str(pipe),   # text summary of the pipeline\n",
    "    # stringified variables for reference\n",
    "    \"variables_str\": str(get_config(\"variables\")),\n",
    "}\n",
    "\n",
    "SCHEMA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(SCHEMA_PATH, \"w\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "print(\"Saved pycaret_setup_config.json\")\n",
    "\n",
    "\n",
    "example_row = test_df.drop(columns=[\"Type\"]).iloc[[0]].to_dict(orient=\"records\")[0]\n",
    "EXAMPLE_REQ_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(EXAMPLE_REQ_PATH, \"w\") as f:\n",
    "    json.dump(example_row, f, indent=2)\n",
    "with open(ARTIFACT_DIR / \"inference_schema.json\", \"w\") as f:\n",
    "    json.dump({\"features\": list(test_df.drop(columns=[\"Type\"]).columns)}, f, indent=2)\n",
    "print(\"Saved example_request.json and inference_schema.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df923ba2",
   "metadata": {},
   "source": [
    "## Manual MLflow logging and model registration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080d529a-37dd-4d6a-973a-d7753ffe346f",
   "metadata": {},
   "source": [
    "**MLflow logging and registry**\n",
    "- The cell logs params, metrics, and artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e52769d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/30 07:05:09 INFO mlflow.tracking.fluent: Experiment with name 'IT3385_WheatSeeds_Task2' does not exist. Creating a new experiment.\n",
      "2025/08/30 07:05:10 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/30 07:05:17 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'WheatSeedsClassifier_Sklearn'.\n",
      "2025/08/30 07:05:17 WARNING mlflow.tracking._model_registry.fluent: Run with id 04216773a8bb49b59aa1284b1daabd30 has no artifacts at artifact path 'model', registering model based on models:/m-016579a0894e4b038536445861a0d210 instead\n",
      "Created version '1' of model 'WheatSeedsClassifier_Sklearn'."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model: WheatSeedsClassifier_Sklearn version: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MLflow logging with new folder structure\n",
    "experiment_name = \"IT3385_WheatSeeds_Task2\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "model_pkl = MODEL_SAVE_STEM.with_suffix(\".pkl\")  # models/wheat_seeds_pipeline.pkl\n",
    "inf_schema = ARTIFACT_DIR / \"inference_schema.json\"  # log if you still create it\n",
    "\n",
    "with mlflow.start_run(run_name=\"wheat_seeds_task2_sklearn\"):\n",
    "    # Params\n",
    "    mlflow.log_param(\"random_state\", RANDOM_STATE)\n",
    "    mlflow.log_param(\"folds\", N_FOLDS)\n",
    "    mlflow.log_param(\"candidate_type\", \"best_tuned/blended/stacked as chosen\")\n",
    "    mlflow.log_param(\"pycaret_version\", __import__(\"pycaret\").__version__)\n",
    "    mlflow.log_param(\"pandas_version\", pd.__version__)\n",
    "    mlflow.log_param(\"sklearn_version\", __import__(\"sklearn\").__version__)\n",
    "\n",
    "    # Metrics\n",
    "    mlflow.log_metric(\"holdout_macro_f1_selected\", float(best_cv_f1))\n",
    "    mlflow.log_metric(\"test_accuracy\", float(test_acc))\n",
    "    mlflow.log_metric(\"test_macro_f1\", float(test_f1_macro))\n",
    "\n",
    "    # Notebook artifacts (tables, split info, plots you saved in ARTIFACT_DIR)\n",
    "    mlflow.log_artifacts(str(ARTIFACT_DIR), artifact_path=\"notebook_artifacts\")\n",
    "\n",
    "    # Inference assets (the files your Streamlit app needs)\n",
    "    for f in [model_pkl, SCHEMA_PATH, EXAMPLE_REQ_PATH, BATCH_PATH]:\n",
    "        mlflow.log_artifact(str(f), artifact_path=\"inference_assets\")\n",
    "    if inf_schema.exists():\n",
    "        mlflow.log_artifact(str(inf_schema), artifact_path=\"inference_assets\")\n",
    "\n",
    "    # Log model to MLflow model registry area\n",
    "    loaded = load_model(str(MODEL_SAVE_STEM))  # e.g., \"models/wheat_seeds_pipeline\"\n",
    "    mlflow.sklearn.log_model(loaded, artifact_path=\"model\")\n",
    "\n",
    "    # Optional registry (works only if tracking server supports registry)\n",
    "    try:\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        model_uri = f\"runs:/{run_id}/model\"\n",
    "        registered = mlflow.register_model(model_uri=model_uri, name=\"WheatSeedsClassifier_Sklearn\")\n",
    "        print(\"Registered model:\", registered.name, \"version:\", registered.version)\n",
    "    except Exception as e:\n",
    "        print(\"MLflow registration skipped or failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f1ad8",
   "metadata": {},
   "source": [
    "## Sanity check: batch prediction using saved pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2fa8e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "      <th>AsymmetryCoeff</th>\n",
       "      <th>Groove</th>\n",
       "      <th>prediction_label</th>\n",
       "      <th>prediction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.07</td>\n",
       "      <td>13.92</td>\n",
       "      <td>0.8480</td>\n",
       "      <td>5.472</td>\n",
       "      <td>2.994</td>\n",
       "      <td>5.304</td>\n",
       "      <td>5.395</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.30</td>\n",
       "      <td>13.34</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>5.243</td>\n",
       "      <td>2.974</td>\n",
       "      <td>5.637</td>\n",
       "      <td>5.063</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.37</td>\n",
       "      <td>13.78</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>5.320</td>\n",
       "      <td>3.128</td>\n",
       "      <td>4.670</td>\n",
       "      <td>5.091</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.01</td>\n",
       "      <td>14.76</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>5.789</td>\n",
       "      <td>3.245</td>\n",
       "      <td>1.791</td>\n",
       "      <td>5.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.11</td>\n",
       "      <td>13.47</td>\n",
       "      <td>0.8392</td>\n",
       "      <td>5.159</td>\n",
       "      <td>3.032</td>\n",
       "      <td>1.502</td>\n",
       "      <td>4.519</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  Compactness  Length  Width  AsymmetryCoeff  Groove  \\\n",
       "0  13.07      13.92       0.8480   5.472  2.994           5.304   5.395   \n",
       "1  12.30      13.34       0.8684   5.243  2.974           5.637   5.063   \n",
       "2  13.37      13.78       0.8849   5.320  3.128           4.670   5.091   \n",
       "3  15.01      14.76       0.8657   5.789  3.245           1.791   5.001   \n",
       "4  12.11      13.47       0.8392   5.159  3.032           1.502   4.519   \n",
       "\n",
       "   prediction_label  prediction_score  \n",
       "0                 1            0.7186  \n",
       "1                 3            0.7849  \n",
       "2                 3            0.6722  \n",
       "3                 1            0.6423  \n",
       "4                 3            0.8452  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_pipe = load_model(str(MODEL_SAVE_STEM))\n",
    "batch_df = pd.read_csv(str(BATCH_PATH))\n",
    "batch_preds = predict_model(loaded_pipe, data=batch_df.copy())\n",
    "batch_preds.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5113e3-309a-4e29-b49b-679f8c83529c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
